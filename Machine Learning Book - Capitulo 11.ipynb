{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de1794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17070a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing database\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7032c7c6",
   "metadata": {},
   "source": [
    "### Training a neural network using leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fe950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0d1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7229e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 1.2819 - accuracy: 0.6229 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7955 - accuracy: 0.7361 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6816 - accuracy: 0.7721 - val_loss: 0.6426 - val_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6217 - accuracy: 0.7944 - val_loss: 0.5900 - val_accuracy: 0.8064\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5832 - accuracy: 0.8074 - val_loss: 0.5582 - val_accuracy: 0.8202\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5553 - accuracy: 0.8158 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5339 - accuracy: 0.8225 - val_loss: 0.5157 - val_accuracy: 0.8302\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5173 - accuracy: 0.8272 - val_loss: 0.5079 - val_accuracy: 0.8284\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5040 - accuracy: 0.8288 - val_loss: 0.4895 - val_accuracy: 0.8390\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4924 - accuracy: 0.8320 - val_loss: 0.4817 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fab67",
   "metadata": {},
   "source": [
    "### Implementing ELU in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd967642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x2c27cd4bc10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df1e3b",
   "metadata": {},
   "source": [
    "### Implementing SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcf790",
   "metadata": {},
   "source": [
    "During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ1 or ℓ2 regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee533775",
   "metadata": {},
   "source": [
    "By default, the SELU hyperparameters (scale and alpha) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278bd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ae5cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e58215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d21b3",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575bb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da88bb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 61s 34ms/step - loss: 1.3466 - accuracy: 0.4814 - val_loss: 1.7223 - val_accuracy: 0.3852\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 60s 35ms/step - loss: 1.2287 - accuracy: 0.5148 - val_loss: 0.8012 - val_accuracy: 0.7128\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 60s 35ms/step - loss: 0.7622 - accuracy: 0.7122 - val_loss: 0.7106 - val_accuracy: 0.7222\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 60s 35ms/step - loss: 0.6481 - accuracy: 0.7567 - val_loss: 0.6208 - val_accuracy: 0.7712\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 58s 34ms/step - loss: 0.6011 - accuracy: 0.7767 - val_loss: 0.5882 - val_accuracy: 0.7938\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5d418",
   "metadata": {},
   "source": [
    "If we use ReLU with the same architecture it will suffer from the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedaec29",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d0967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecbab9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01108631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45aa1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.8750 - accuracy: 0.7122 - val_loss: 0.5523 - val_accuracy: 0.8222\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.5753 - accuracy: 0.8030 - val_loss: 0.4724 - val_accuracy: 0.8470\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.5189 - accuracy: 0.8204 - val_loss: 0.4374 - val_accuracy: 0.8546\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4827 - accuracy: 0.8325 - val_loss: 0.4151 - val_accuracy: 0.8602\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4565 - accuracy: 0.8408 - val_loss: 0.3996 - val_accuracy: 0.8642\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4397 - accuracy: 0.8471 - val_loss: 0.3867 - val_accuracy: 0.8692\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4242 - accuracy: 0.8513 - val_loss: 0.3764 - val_accuracy: 0.8704\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4143 - accuracy: 0.8541 - val_loss: 0.3712 - val_accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4023 - accuracy: 0.8581 - val_loss: 0.3631 - val_accuracy: 0.8758\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.3914 - accuracy: 0.8622 - val_loss: 0.3573 - val_accuracy: 0.8754\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a9975",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ca00b",
   "metadata": {},
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "\n",
    "X_train_A: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "X_train_B: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "706e4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e323d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b251d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69307b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d0219d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.5927 - accuracy: 0.8101 - val_loss: 0.3897 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.3523 - accuracy: 0.8784 - val_loss: 0.3289 - val_accuracy: 0.8819\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.3170 - accuracy: 0.8896 - val_loss: 0.3014 - val_accuracy: 0.8989\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2972 - accuracy: 0.8973 - val_loss: 0.2892 - val_accuracy: 0.9018\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2834 - accuracy: 0.9021 - val_loss: 0.2774 - val_accuracy: 0.9058\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2728 - accuracy: 0.9062 - val_loss: 0.2731 - val_accuracy: 0.9073\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2641 - accuracy: 0.9092 - val_loss: 0.2717 - val_accuracy: 0.9088\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2572 - accuracy: 0.9126 - val_loss: 0.2588 - val_accuracy: 0.9138\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2518 - accuracy: 0.9134 - val_loss: 0.2560 - val_accuracy: 0.9145\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2468 - accuracy: 0.9153 - val_loss: 0.2539 - val_accuracy: 0.9163\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2422 - accuracy: 0.9179 - val_loss: 0.2494 - val_accuracy: 0.9150\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2382 - accuracy: 0.9188 - val_loss: 0.2509 - val_accuracy: 0.9126\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2350 - accuracy: 0.9197 - val_loss: 0.2444 - val_accuracy: 0.9150\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2314 - accuracy: 0.9213 - val_loss: 0.2411 - val_accuracy: 0.9173\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2286 - accuracy: 0.9211 - val_loss: 0.2447 - val_accuracy: 0.9183\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2253 - accuracy: 0.9226 - val_loss: 0.2384 - val_accuracy: 0.9185\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2229 - accuracy: 0.9233 - val_loss: 0.2405 - val_accuracy: 0.9173\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2199 - accuracy: 0.9244 - val_loss: 0.2420 - val_accuracy: 0.9153\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2177 - accuracy: 0.9255 - val_loss: 0.2330 - val_accuracy: 0.9200\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2154 - accuracy: 0.9262 - val_loss: 0.2331 - val_accuracy: 0.9203\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea41b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a215174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8660d4d",
   "metadata": {},
   "source": [
    "Note that model_B_on_A and model_A actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build model_B_on_A on top of a clone of model_A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb6cf719",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "model_B_on_A = keras.models.Sequential(model_A_clone.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "400c7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Al ppio se freeza el entrenamiento de las capas reutilizadas durante algunas epocas para darle tiempo a las capas\n",
    "# sin entrenar de aprender pesos representativos\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e5eb0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 35ms/step - loss: 2.2058 - accuracy: 0.1200 - val_loss: 2.0775 - val_accuracy: 0.1927\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 2.0266 - accuracy: 0.1600 - val_loss: 1.9250 - val_accuracy: 0.2241\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.8670 - accuracy: 0.1950 - val_loss: 1.7802 - val_accuracy: 0.2546\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.7161 - accuracy: 0.2300 - val_loss: 1.6428 - val_accuracy: 0.2890\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.2177 - accuracy: 0.3400 - val_loss: 0.7654 - val_accuracy: 0.4807\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.6158 - accuracy: 0.6050 - val_loss: 0.5251 - val_accuracy: 0.7069\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.4191 - accuracy: 0.8000 - val_loss: 0.3891 - val_accuracy: 0.8489\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.3085 - accuracy: 0.9250 - val_loss: 0.3079 - val_accuracy: 0.9097\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.2411 - accuracy: 0.9450 - val_loss: 0.2545 - val_accuracy: 0.9310\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1963 - accuracy: 0.9650 - val_loss: 0.2192 - val_accuracy: 0.9462\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1667 - accuracy: 0.9700 - val_loss: 0.1933 - val_accuracy: 0.9564\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.1445 - accuracy: 0.9850 - val_loss: 0.1731 - val_accuracy: 0.9604\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1269 - accuracy: 0.9900 - val_loss: 0.1569 - val_accuracy: 0.9655\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1129 - accuracy: 0.9950 - val_loss: 0.1448 - val_accuracy: 0.9675\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.1022 - accuracy: 0.9950 - val_loss: 0.1335 - val_accuracy: 0.9696\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0927 - accuracy: 0.9950 - val_loss: 0.1249 - val_accuracy: 0.9706\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0849 - accuracy: 0.9950 - val_loss: 0.1176 - val_accuracy: 0.9706\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0784 - accuracy: 0.9950 - val_loss: 0.1106 - val_accuracy: 0.9757\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0722 - accuracy: 0.9950 - val_loss: 0.1051 - val_accuracy: 0.9777\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.0673 - accuracy: 0.9950 - val_loss: 0.1004 - val_accuracy: 0.9787\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "15eaae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0986 - accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0985935777425766, 0.9815000295639038]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226561a",
   "metadata": {},
   "source": [
    "Con el modelo B entrenado de 0, el accuracy seria 0.97, mientras que aca es 0.98 (en el libro da 0.99 - dice que probo la mejor estructura). Tener en cuenta que transfer learning no funciona bien para redes densas pequeñas.\n",
    "Transfer learning works best with deep convolutional neural networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a884ee6",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12096698",
   "metadata": {},
   "source": [
    "## Momentum optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a9315",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bc67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b50f9d",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc61b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c95427",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4819db",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d7338",
   "metadata": {},
   "source": [
    "## Adam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd0ac9b",
   "metadata": {},
   "source": [
    "## Adamax Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ca8bc6",
   "metadata": {},
   "source": [
    "## Nadam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496bf915",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d38c7",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b576f",
   "metadata": {},
   "source": [
    "Implementing power scheduling in keras is the easiest option: just set the decay hyperparameter when creating an optimizer.\n",
    "The decay is the inverse of s (the number of steps it takes to divide the learning rate by one more unit), and Keras assumes that c is equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7623e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251f79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ceed28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 13s 6ms/step - loss: 0.4855 - accuracy: 0.8282 - val_loss: 0.4183 - val_accuracy: 0.8586\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3777 - accuracy: 0.8664 - val_loss: 0.3782 - val_accuracy: 0.8734\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3449 - accuracy: 0.8773 - val_loss: 0.3690 - val_accuracy: 0.8714\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3239 - accuracy: 0.8849 - val_loss: 0.3625 - val_accuracy: 0.8798\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.3081 - accuracy: 0.8897 - val_loss: 0.3513 - val_accuracy: 0.8760\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2947 - accuracy: 0.8947 - val_loss: 0.3529 - val_accuracy: 0.8742\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2850 - accuracy: 0.8974 - val_loss: 0.3411 - val_accuracy: 0.8848\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2750 - accuracy: 0.9014 - val_loss: 0.3483 - val_accuracy: 0.8796\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2675 - accuracy: 0.9039 - val_loss: 0.3431 - val_accuracy: 0.8828\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2605 - accuracy: 0.9075 - val_loss: 0.3332 - val_accuracy: 0.8862\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2542 - accuracy: 0.9096 - val_loss: 0.3316 - val_accuracy: 0.8850\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2483 - accuracy: 0.9113 - val_loss: 0.3337 - val_accuracy: 0.8878\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2431 - accuracy: 0.9135 - val_loss: 0.3314 - val_accuracy: 0.8852\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2385 - accuracy: 0.9157 - val_loss: 0.3303 - val_accuracy: 0.8870\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2336 - accuracy: 0.9167 - val_loss: 0.3347 - val_accuracy: 0.8862\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2297 - accuracy: 0.9185 - val_loss: 0.3338 - val_accuracy: 0.8858\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2261 - accuracy: 0.9206 - val_loss: 0.3286 - val_accuracy: 0.8876\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2223 - accuracy: 0.9221 - val_loss: 0.3266 - val_accuracy: 0.8866\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2187 - accuracy: 0.9228 - val_loss: 0.3253 - val_accuracy: 0.8904\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2157 - accuracy: 0.9243 - val_loss: 0.3290 - val_accuracy: 0.8860\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2132 - accuracy: 0.9260 - val_loss: 0.3276 - val_accuracy: 0.8870\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2101 - accuracy: 0.9268 - val_loss: 0.3254 - val_accuracy: 0.8888\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2074 - accuracy: 0.9276 - val_loss: 0.3313 - val_accuracy: 0.8886\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2047 - accuracy: 0.9278 - val_loss: 0.3284 - val_accuracy: 0.8886\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2023 - accuracy: 0.9299 - val_loss: 0.3293 - val_accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "627f3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver mas opciones de scheduling en el libro o en el notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003fb31",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e870a20",
   "metadata": {},
   "source": [
    "### l1 and l2 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e644ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor of 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660e0c3",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac9dd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.5661 - accuracy: 0.8032 - val_loss: 0.3769 - val_accuracy: 0.8596\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4234 - accuracy: 0.8450 - val_loss: 0.3520 - val_accuracy: 0.8688\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff85ba7",
   "metadata": {},
   "source": [
    "### Max norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854fff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402afd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50597d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
